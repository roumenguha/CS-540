One Hundred Year Study on Artificial Intelligence - A Rebuttal
---------------------------------------------------------------

The article published by Stanford goes over various aspects of Artificial Intelligence (AI) and speculates how AI might affect the society as a whole and also goes into detail of how AI might change specific sectors of the society. The article also provides some guidelines on what might have to be considered as our society adapts to presence of AI in various parts of our lives. In addition, it raises some concerns over biases and how these biases may slowly get incorporated into AI programming. It takes a stance that biases must be removed from AI as much as possible so it can make objective decisions in critical situations. However, biases are a huge part of humanity. While not all biases are helpful, many biases help us make decisions that are aligned to our moral compass. Moreover, these biases change over time, along with our moral views.

Let us begin by considering why humans have biases. Biases happen when our brain, due to experience, training, or faulty thinking, takes a shortcut in decision making, making the process faster. For example, hitting a nail with hammer causes an entire range of reactions in different individuals, from flinching to not even blinking. Someone not exposed to the sound and vibration of a hammer hitting a nail will flinch, even though the rational part of brain knows no harm can come from it. On the other side, someone who has experience with the sound and vibration may not even blink to the sound. Although the “rational” part of both individuals’ brains send out same messages, their experience, or in other words, biases cause them to react differently. Although this example is not relevant to AI, it shows how biases can change our reactions and how experience can change our biases. 

But how do biases connect to AI? Biases are very good indicators of where the moral compass of our society points. It is our biases that forces us to swerve away from a pedestrian, even if it mean putting our own assets and lives in danger; it is what causes us to save children first even when they are not the logical choice to save. The article hypothesizes that AI will be used to make increasingly critical decisions, from preventing accidents on roads to deciding whether or not to open fire at a place filled with civilians. Systematically removing biases from AI will also make it extremely objective. Our society does not measure human lives objectively. Ideally, loss of life should be minimized even if the results are suboptimal. This lack of objective measure for human life is the idea behind the Trolley Problem. A problem that Self-Driving cars face even now. As of yet, there is no solution to the problem. There are very good arguments on both sides, but with no moral compass to point the way, decisions are hard to come by. 

This brings me to my point: The article makes recommendation that while programming AI, care should be taken to remove any bias from the algorithms. While we probably do not want our AI to have cognitive biases, removing all biases from AIs might not be as good an idea until we learn to quantify our own biases. Although biases get a lot of negative publicity when connected to racially charged and other similar incidents, it is often forgotten that a lot of correct crucial decisions in our lives are made because of our biases. While it is prudent to remove the bias that targets a specific color of people for policing, we probably do not want to remove the bias that prioritizes saving children, even hazarding our own lives in the process. The question about which biases to keep, and which to remove, is a question that requires discussion at a larger scale. But ultimately, removing all biases from an AI will make it less acceptable to the society.
