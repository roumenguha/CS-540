{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf830
{\fonttbl\f0\froman\fcharset0 TimesNewRomanPSMT;}
{\colortbl;\red255\green255\blue255;\red38\green38\blue38;\red255\green255\blue255;\red0\green0\blue0;
\red120\green9\blue17;}
{\*\expandedcolortbl;;\cssrgb\c20000\c20000\c20000;\cssrgb\c100000\c100000\c100000;\csgenericrgb\c0\c0\c0;
\cssrgb\c54902\c8235\c8235;}
\margl1440\margr1440\vieww13520\viewh18020\viewkind0
\deftab720
\pard\pardeftab720\sl300\sa40\partightenfactor0

\f0\fs24 \cf2 \cb3 \expnd0\expndtw0\kerning0
While Stanford\'92s One Hundred Year Study on Artificial Intelligence is overall very well-written and forward-thinking, I believe the authors heavily glossed over the topic of security as well as made significant assumptions and misstatements about the inherent biases associated with human-decision making, especially on policing techniques.\
\
In the opening paragraph of the Public Safety and Security section, the authors emphasize how important gaining the public\'92s trust is, however make a large assumption with \'93assuming careful deployment\'94 and go on to discuss that, \'93AI may also remove some of the bias inherent in human decision-making.\'94 While I whole-heartedly agree that there are many examples of egregious prejudices and biases that plague human decision-making, I believe that removing some of the biases associated with activities such as policing could inhibit or halt the learning process entirely, resulting in sub-optimal machines or devices. The relationship with the public and proper policing AI will be that of give and take, but if the optimal AI is out there, the public will need to sacrifice some of its freedoms for the overall good.\
\
Moreover, in the second paragraph of the Public Safety and Security section, the authors discuss how improvements in machine learning and transfer learning have been sped up by the machines learning in new scenarios based on similarity with past scenarios. Wouldn\'92t making decisions in the present because of a particular event in the past be classified under some sort of bias? Isn\'92t learning built entirely on acting on what you know? Where do you draw the line when an artificially intelligent machine acts in the best interest of the public, but exhibits some sort of bias for or against a certain group or idea? The report states that \'93Machine learning significantly enhances the ability to predict where and when crimes are more likely.\'94 Wouldn\'92t the demographic inhabiting high-crime areas feel as if the policing AI being used is exhibiting a bias on them or would they feel more safe knowing that the machine has their safety in its best interest and be thankful? Similarly, can the public sacrifice its pride and not feel personally attacked if AI algorithm profiles them? For instance, if an innocent individual is stopped at an airport security line and is profiled by an artificially intelligent machine, could the individual stomach the shot to his or her ego and let the corrective nature of the algorithm learn from its mistakes and better protect the public? These are the types of situations in society that would need advancing prior to large leaps in AI.\
\
The authors are making a giant leap by stating that \'93well-deployed AI\'94 can tie up all those loose ends and gain the public trust without explicitly defining what exactly that entails or what the course of action could be to implement it.  Some of the shortcomings of AI fall within the realm of shaping the machine around some of the ever-changing societal values and not overstepping or encroaching on some of the freedoms a lot of citizens enjoy today. I do believe that one day there will be AI capable of policing properly, however I do not think that it will be without many biases due to the nature of learning based on past experiences and that the terms \'93careful deployment\'94 as well as \'93well-deployed\'94 used in the report are overly optimistic and unrealistic given the world we live in today.\
\
\
\cf4 \cb1 Works Citied:\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \outl0\strokewidth0 \strokec2 Peter Stone, Rodney Brooks, Erik Brynjolfsson, Ryan Calo, Oren Etzioni, Greg Hager, Julia Hirschberg, Shivaram Kalyanakrishnan, Ece Kamar, Sarit Kraus, Kevin Leyton-Brown, David Parkes, William Press, AnnaLee Saxenian, Julie Shah, Milind Tambe, and Astro Teller. \'a0"Artificial Intelligence and Life in 2030."\'a0One Hundred Year Study on Artificial Intelligence: Report of the 2015-2016 Study Panel, Stanford University, Stanford, CA, \'a0September 2016.\'a0Doc:\'a0{\field{\*\fldinst{HYPERLINK "https://ai100.stanford.edu/2016-report"}}{\fldrslt \strokec5 http://ai100.stanford.edu/2016-report}}. Accessed: \'a0September 11, 2016.}