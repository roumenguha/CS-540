In General, the predictions made by the ‘100 year study of AI’ are accurate. But some predictions about AI in the next 15 years are too extreme. The section on public safety/security is particularly unbelievable. The section contains predictions stating that AI will determine possible criminal activity based off behavior trends seen from security cameras or through posts on social media. But these predictions, in my opinion, will likely not come to fruition. The study is wrong in that AI will not correlate suspiciousness and behavior in the next 15 years. If any such program was designed and implemented into society for this purpose, it would be a complete breach of privacy and very inaccurate.
        The program would be inaccurate because correlating suspiciousness and behavior is nigh impossible. People come in all shapes and sizes; our behavior is subject only to our physiology and standards. Therefore, the variables involved in determining how a suspicious person differs from the “standard” person are too numerous and obscure to properly utilize. The following quote from the study illustrates signals AI could use to determine : “vision, speech analysis, and gait analysis— can aid interviewers, interrogators, and security guards in detecting possible deception and criminal behavior.” A program utilizing these signals alone would not be capable of detecting deception. For example, Gait analysis is flawed because one may walk strangely if suspect, but one may also walk strangely due to any sort of leg injury. This would make the program inherently biased against the less mobile. Many other “signals” fall short in the same way. Implementing this “behavioral analysis AI” would be little more accurate than a lie detector. Another thing AI cannot account for is humanity.
        People act differently when being analyzed. While under AI scrutiny, one will try to act “normal”, but will inevitably become nervous. Ironically,  nervousness may even trigger the AI meant to detect suspicious behavior. It’s a classic incorrect pairing of correlation with causality. On surveillance, the study states the following: “These improvements could lead to even more widespread surveillance. Some cities have already added drones for surveillance purposes … raising concerns about privacy, safety, and other issues.” Behavioral analysis AI would not only be inaccurate, but also oppressive. If surveillance was widespread enough, one would need to live their whole life “normally” to avoid raising their “person’s risk categorization” (quoted from study). The constant surveillance of a population and a correlating risk categorization would be invasive, even oppressive. In addition to constant surveillance, AI could risk privacy online as well.
        Social media tracers (AI that utilize posts on social media to determine possible criminal activity) may also be developed. This AI would be incredibly invasive. Having one’s private profiles analyzed for suspicious activity creates an atmosphere of constant supervision. In addition, Social media tracers are inaccurate. AI is far from understanding all the nuances of even english - especially the slang and irony found on most social media. For example, Nuance could make it incredibly difficult for AI to discern between a real threat and a distasteful joke. This, paired with the invasive tech, would inevitably end up with innocent people being monitored just because of an inappropriate post. 
        Overall, AI is far too underdeveloped for use in monitoring human behavior. The technology, if created and implemented today, would be incredibly inaccurate and invasive. These factors make AI technology oppressive and unfriendly. If this were how the public first experiences modern AI, the technology would wallow. It’s as the article says: humans must trust AI before AI can become successful. For the purpose of advancing AI, monitoring human behavior to determine suspiciousness should not be implemented until reliable and noninvasive technology is developed.