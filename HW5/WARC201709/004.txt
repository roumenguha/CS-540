The Stanford Report, among their list of goals, gives:

"Remove the perceived and actual impediments to research on the fairness, security, privacy, and social impacts of AI systems." (pg. 43)

Both public and private enterprises must reserve the right to install whatever impediments to research they see fit. A lack to do so would allow the inadvertent collection of sensitive data. A lack to do so would stifle incentive research on AI systems. A lack to do so would reduce public support for AI systems. Through the examination of current examples and historical precedence we will see that The Stanford Report's goal is misguided.

Impediments on arbitrary access AI systems must be preserved to prevent access to private data. Amazon collects large amounts of data both on what people have purchased and what people seek to purchase in the future. These data contain private data concerning individual persons' purchases. Amazon uses AI systems to produce services such as 'you may also like:' based on persons' previous purchases. These AI systems are based on if not containing data that people entrust to Amazon to keep private from external parties. If impediments to access these data are not present it is conceivable that Amazon's AI systems containing these personal data may be abused.

Impediments on arbitrary access to AI systems must be preserved to prevent stifling of incentive to research on AI systems. Google spends large amounts of money researching AI (look no further that their purchase of Deep Mind). On top of the initial research they spend large amounts of money on time and computing energy to progress their AI systems. If they are not allowed to create impediments to their AI, then their highly invested AI systems would be available for any other enterprise to use without substantial cost. If this is true, then Google would have very little incentive to spend large amounts of money on their AI research. This would decrease private research on AI, counter to the report's third goal.

Impediments on arbitrary access to AI systems must be preserved to prevent public backlash to AI. People, that is groups of people -- not individual persons, are easily frightened. If an AI system presents a result that is controversial, we will quickly grow sour opinions of AI systems and cast unnecessary reservation on them; it would be very similar to our aversion to nuclear power based on Three Mile Island, Fukushima, and Chernobyl. If all AI systems are made publicly available, it is very likely that a journalist may take the most cynical view of a particular AI result, and it would lead to a public outcry against AI. Impediments must be preserved to prevent unnecessary outcry to AI.

If AI research is to continue, and to continue fruitfully, it is imperative that we maintain a private incentive to do research on it, and that we leave the science to those who know how to properly interpret the results of AI. A precondition to these is impediments installed against arbitrary access to AI systems, and therefore the Stanford Report is wrong on this second goal.

