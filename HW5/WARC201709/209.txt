In the report of Stanford 100 year study on artificial intelligence, the author mentioned that there will be auto driving cars in the future. And ethical and legal programs will arise, so the cars need to be programmed in a fashion to avoid such problems. However, is it really a good idea to hand over ethical problems to machines and artificial intelligence? Maybe it is okay to hand over some small decisions to machines, but for some important decisions that may cause someone to die, can we still rely on some programmed machines? For example, if an auto driving car is on the road and someone started trespassing the road. It is already too late to stop the car. Will the car hit the trespasser, or will the car run over an innocent bystander? Or will the car run into a wall and put its owner’s life at risk? It is a very tough decision. For us human, if such a situation really happened we may not have enough time to think thoroughly to make the decision, but a machine probably have time to do enough calculations to make its decision. Let’s assume that we have time, which of the three choices will we choose? It is even a tough call for us. Should we run into the trespasser because it’s his fault to cross the road at the wrong time? What is a group of people is trespassing? This is a question with no correct answer. No matter which choice the car makes, someone will get hurt, or possibly die.
If it is even such a hard choice for us, how can we hand over the decision of letting who to live into a machine’s mind so easily? This situation is just an example, but we must assume worse things can happen in real life, and make the machine can handle whatever rough situations that are thrown at it. If someone is killed in an incident like this one, who should be responsible? It is the trespasser who started the problem, but it is the car that made the decision of who or what to hit. Some cases might be very ambiguous and a very tough call for the judge to decide who should be responsible. There is still a lot of things to think and a lot of rules to make before we can mass implement technologies like auto driving. While researching such technologies, we should foresee similar issues like this one and establish some ground rules. Of cause the rules will not be perfect, but it will at least put some restrictions on the companies seeking business in this field. This will stop some potential exploits. These rules and laws can be perfected overtime, both before and after the technology itself become mature. At that time, auto driving will probably be fine in most situations but when facing such an ethical dilemma, will a protocol for the AI ever be developed to handle a situation like this? 
