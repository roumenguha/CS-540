      The Stanford 100-year study on AI aims to predict and guide use of Artificial Intelligence within society. It did so by focusing on societal trends and technological adoption within US cities, and looking ahead to the year 2030. I argue that by focusing on cities only, the study overlooked and underestimated significant elements of resistance within society at-large which will hamper some of the predictions within the study. 
      Within the transportation sector, the study predicted that autonomous vehicles would significantly alter lifestyles by becoming the dominant mode of transportation. While I would accept the proposition that public transit will be significantly enhanced through AI, rural communities who do not rely on public transit today, or who have insufficient funding to even maintain their roads, will struggle to adopt autonomous transportation. A typical vehicle produced since 2000 already has an expected lifespan of 15 years or more, which means vehicles on the road today will still be on the road in 2030. Having those millions of vehicles on the road will force public policy to continue to accept human-driven vehicles, and all the additional legal and safety complexities associated with the interaction of AI and human drivers. We also cannot overlook that in many parts of US culture, driving is closely associated with power and self-importance: see marketing directed at buying powerful trucks to work on farms and construction. These cultures often have underlying resistance towards strong central government as well, specifically where personal privacy is concerned. AI transportation networks would be increasingly limited if they are unable to communicate and coordinate with other vehicles within the network, which limits the overall possible gains from such a system. As with many other technologies, we're more likely to see better AI transit adoption in pockets (university systems, strong centralized governments such as Sweden) throughout the world, but it will take longer than 15 years for society at-large to catch up.
      The study also made predictions in the public safety sector, such as the concept that AI can assist in "predictive policing" and crime prevention. The study itself acknowledged repeatedly that this concept is prone to institutional discrimination, but I believe it fell short in predicting just how large a barrier this is to progress. We need only look at the news the last couple months to see incident after incident of alleged police brutality and racial profiling. While at a technical level I would completely agree that it's possible to design "neutral and objective" algorithms to assist law enforcement, who exactly would be in a position to enforce that standard? How would that organization earn the public's trust enough that cultures, with decades of history of being discriminated against, believe that the system is inherently fair? Repeating the privacy concern, I do not believe rural/poor communities with a history of resisting strong central government will willingly allow a database to be created to effectively facilitate predictive law enforcement. The study can make light of Minority Report type situations, but reality is people will assume exactly that is going on, and the efforts to earn public trust will be slow and lengthy.
      Across all sectors, similar claims can be made. People today discriminate against healthcare professionals who do not share their race, but we hope they will trust a computer to give them medical advice? With the divisive nature of politics in the country, we hope an agreement can be reached regarding AI-generated wealth? These are great thoughts and dreams, but there is much yet to overcome before they can become reality.
