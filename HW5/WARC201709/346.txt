      The Stanford One Hundred Year Study on Artificial Intelligence provides a detailed insight into the current progress of the field of AI and attempts to provide guidance for future directions in AI. In the recent years, Artificial Intelligence (AI) has been under the spotlight for a myriad of reasons, of which, the powers of AI and its potential as a threat to humans seemed to be of concern to the general populations. 
      The 2016 study seems to particularly focus on this aspect; the report stated that there was “no cause for concern that AI is an imminent threat to humankind”. It goes on to state that there are have not been and there are no intentions to build a machine with “self-sustaining long-term goals”. This statement seems to be problematic as it fails to consider the implication and the possibility of the creation of a machine that self-learns to be sufficient unbeknownst to its creators. 
      The report appears to focus mainly on the benefits of implementation and utilization of AI but fails to consider the long-term consequences of a potential rogue AI system, where humans have no control of. While the report suggests that strict governmental regulations should be put in place to monitor the progression of AI and to cope with the changing legal landscape brought upon, it fails to discuss the necessity of finding and creating a robust solution to retaining full control of all AI systems. This concern is also raised in the Open Letter on Artificial Intelligence signed by industry leaders of AI where other challenges such as verification, validity and security are also raised. This notion of retaining full control over AI is to ensure that future goal-based AI programs do not resist modifications to goal specifications from engineers after launch. 
      Creation of a “friendly artificial intelligence” is more important and crucial at this point; practicality and constraints towards building an ethically behaved AI should be explored. Supporters of “friendly AI” call for it as they believe in its creation as a form of mitigating existential risks that advanced AI might pose. This idea of creating a “friendly AI” is to explicitly counteract any sufficiently advanced AI system from exhibiting undesired behavior. Consider the recent Facebook AI program that invented its out language; a research algorithm that deviated from human language, which showcased the possibility of what can happen with unconstrained feedback. Similarly, suggestions from machine ethics researcher, Bruce Schneier, recommends the adoption of a “security mindset”; by which preparing for the failure of a system is of greater importance than the success of the system. This mindset similarly echoes Isaac Asimov’s Three Laws of Robotics where he proposed safety measures to prevent robots from harming humans. 
      Overall, the study provides a decent insight into the developments of AI, but perhaps a more detailed consideration of the aspects of “friendly AI” is necessary to sooth worries. It might be speculative to focus on the existential risk from AI, however, it is important for engineers and AI developers to acknowledge these potential risks and practice precaution while putting in place systems of checks and works towards prevention.
