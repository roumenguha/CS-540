Stanford 100 Year Study Essay

Reading through the Stanford 100 Year Study On Artificial Intelligence, I came across a few points I disagreed with. The points I specifically had issue with were their vision of car ownership, privacy concerns in health data, and the audit-ability of AI. In the paper, they state that by 2020 autonomous vehicles will be widely adopted. 
Based on the amount of autonomous vehicles on the road today, and the regulation surrounding they I find it very hard to believe that in just over two years they will achieve wide adoption. By 2030, they predict decreased car ownership in cities. Instead they believe people will rent out the equivalent of car timeshares in autonomous vehicles. I am unconvinced for two reasons, one most car traffic happens at two defined times per day which we generally refer to as “rush hour”. During this time every single working person will need access to a car, but during other times there will be extreme downtimes that only a very small percentage of the vehicles will be used. I find it hard to believe that people will pay enough to rent out these time shares to make a company profit while still owning enough cars to meet the demand. The second reason is that humans desire ownership. If we need to have a vehicle to get around in every day, most people would rather have one that they own. Even when cars are leased or homes are rented today, people still act like they own them, decorating and customizing them. 

Another issue I had with the paper is their lack of concern for potential privacy issues. The writers state multiple times that current regulations in the healthcare industry are preventing great strides forward in AI and medical diagnosis. While I agree that removal of regulations like HIPPA may improve diagnosis, it also leads to huge problems in patient privacy. Even if the data in anonymized, only so much information can be removed while still getting relevant information for medical use. As anyone in the security and cryptography sector knows, meta data can reliably be used to identify users so it’s not hard to imagine a situation where a patient could be identified using only the anonymized data. Combined with the regular data breaches due to lazy or negligent security practices all across the private sector, the most recent one being the Equifax breach, the much data in one place is a disaster waiting to happen.

The final issue I had with the Stanford paper is their claim that AI are inherently more audit-able than humans. Artificial intelligences based on neural networks can be very difficult to audit, as the ai may use similarities to pick out certain patterns that aren’t obvious to us or are obviously not true for all systems. An example of this is the google deep dream project, where they found that their image processing network thought a forearm and dumbbell were a single object. Careful edge testing will have to happen in potentially dangerous systems to ensure AI don’t make these “obvious” mistakes. 
https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html