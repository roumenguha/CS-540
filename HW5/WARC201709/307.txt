Home/Service robots are  predicted to do many tasks with advancements  in artificial intelligence snd mechanics, which  will make it possible for them to move freely in unstructured environments. But these robots are are not safe because  they are vulnerable to security flaws. These robots have  weak cryptographic systems and  authentication issues. These make them unsafe to be used at homes as lot of data can be lost if these robots are hacked. For instance,  robots in factories if hacked can bring down an entire assembly line. This is what happened when a ransomware attack targeted  robotic assemblers in a Mexican plant.
AI  is  being used widely by MOOCS  for automatic assignment grading.There has not been general acceptance for the use of artificial intelligence technologies within automated grading systems. The  recent moves by online education organizations to use artificial intelligence technologies for high-stakes testing has raised eyebrows. This has resulted  in an online petition against automatic scoring of essays by concerned academia. The argument is that machines  cannot measure the essentials of effective written communication like  accuracy, reasoning, adequacy of evidence,  convincing argument, meaningful organization,clarity and veracity. Les Perelman, a researcher at MIT, is highly critical of automated grading systems and  states that “comparing the performance of human graders matching each other to the machines matching the resolved score still gives some indication that the human raters may be significantly more reliable than machines.In June 2012, Perelman submitted a nonsense essay to the US Educational Testing Service’s (ETS) automated grading systems called e-Rater and received the highest possible grade.ETS uses the e-Rater software, in conjunction with human assessors, to grade the Graduate Record Examinations (GRE) and Test Of English as a Foreign Language (TOEFL), without human intervention for practice tests.Both These tests decide the entrance to US graduate schools and the latter the fate of non-English speakers wishing to study at American universities.Though MOOCS may be using machine learning and NLP for automatic grading, this practice is highly contested.
Using AI in healthcare is not completely safe.AI diagnosis work will require access to vast quantities of data in medical records. But what if it were subsequently misused?.A report by the Royal Society and the British Academy recently concluded that the collection and analysis of data is changing so rapidly that the UK’s systems of governance cannot keep up. It concluded that a new body is needed to safeguard trustworthiness and trust.The UK Information Commissioner’s Office (ICO) reprimanded the Royal Free Foundation NHS Trust over an agreement struck with Google DeepMind because  the deal gave the AI company access to 1.6 million people’s medical records to develop a monitoring tool for kidney patients: the ICO ruled that they were not properly informed about the use of their data, among other shortcomings. Automation bias is another problem of using AI in medical  diagnosis. Automation bias is the  tendency to  not search for contradictory information in light of a computer-generated solution that is accepted as correct .The study of automation bias in medicine  has become  relevant with the new machine learning approaches entering clinical decision support. 
