	Reading “Artificial Intelligence and Life in 2030” exposed the possibilities of artificial intelligence today and the promises of tomorrow. While I enjoyed learning about the huge strides of AI over the past 15 years I was troubled by how the report handled the future of AI. The report talked about some concerns of AI as we progress in the field, but I still found myself skeptical about the definition of AI and nervous about future implementation of the technology. 	Artificial intelligence is widely used today, but many people do not see the advantages because it is poorly defined. Factory workers see a mechanical arm or cashiers see an automated cash register and are immediately against AI. Even if AI is not used in the machines that replace workers, many people associate AI with machines taking over the world because the field has done a poor job of defining itself. This is an issue for the future of artificial intelligence and the report needs to identify the problem. Stating “AI will likely replace tasks rather than jobs” (8) and broadly defining AI, as done in the report, is not enough. Misunderstandings about AI will lead to unnecessary regulations and roadblocks hurting future development of this vital technology. 	A misguided definition of AI will hurt the future development, but poor or early implementation today will also cause panic and calls for regulation. When Tesla released its autopilot to the world, videos of drivers with no hands on their steering wheels went viral. This caused a chain reaction of people attempting more and more dangerous activities while driving with the autopilot activated. This is a prime example where a company failed to regulate itself, thus hurting the industry. There is still much unknown about self-driving vehicles and their reliability. There was already one accident involving a self-driving Tesla, how many does it take before the public calls for more regulation? Companies need to self-regulate and prove their artificial intelligence is safe and reliable. The report suggests government regulation is necessary, but I find that inefficient, leading to slower growth in the artificial intelligence industry. 	Artificial intelligence is a difficult field to define. The technology has applications across many fields and that wide spread use worries many people. Currently, little is being done to inform the public about the advantages of AI. The report “Artificial Intelligence and Life in 2030” gives a broad definition of AI, but this is not enough. Until the field of AI is clear with the public about its purposes the public will be skeptical with implementation. This skepticism is also fueled by the unknown performance of some AI technologies today. The report believes government regulation will be necessary for AI in the future, but too much regulation often causes a slow in growth. To prevent this slow down, the industry needs to self-regulate new technologies until extensive testing is completed. Otherwise, people will only focus on the negatives of AI and not the positives. Instead of the report focusing on what the future will hold in AI we need to better define the field today and center the attention on creating reliable and well understood technologies.