Although this article attempts to perform a very thorough overview of the history, the current progress, and the future of artificial intelligence, it seems to make assumptions about the current status of the A.I. in order to predict its future. More specifically, Section II referring to AI in transportation appears to have issues in regard to handling all scenarios that a human would face. In short, there rises a question on how AI’s would handle paradoxes as a result of being forced to make ethical, split second decisions when used as modes of transportations, such as self-driving cars. The question raised today is this: in a situation where a self-driving car must make an ethical decision, should the self-driving car either 1.) run over numerous people by going straight and saving the driver of the vehicle or to 2.) steer away from running over numerous people and crashing into an obstacle effectively killing the driver?
First, human biases in regard to the development and learning of artificial intelligence must be addressed. As Google as noted in one of their recent ads, every machine learning model and artificial intelligence is inherently prone to biases humans have, whether referring to the data that is fed for the A.I. to train on or the developer that’s teaching the A.I. what is right and wrong. This inherent issue in regard to biases that all A.I.’s will develop is an issue that has compounding effects on daily, consumer applications of A.I. where they will be heavily relied on. More specifically, how an A.I. is trained to perform will be the ultimate deciding factor of which ethical decision it will make. While many people might believe that A.I. should or shouldn’t run over the individuals on the road, it is ultimately left to how the A.I. was trained, thus proving that A.I. is inherently subjective to its creator.
One may argue that an A.I. should be programmed to save the most lives as possible, thus rendering a clear and defined protocol for the A.I. to abide by. This, however, will create another ethical issue of its own. Let’s say that all A.I. are programmed to save the most lives as possible, so in the scenario provided above, all driverless cars will opt for solution (2), where the driver will be killed. This might seem like choosing from the best of two evils, but as a consumer considering purchasing self-driving cars, knowing that the A.I. inside would rather kill you than save your life would effectively discourage you from buying that car. This could result in overall decreased interest in self driving cars which could have devastating effects on the progression of mankind. 
However, in a more likely scenario, automobile manufacturers would foresee this issue and actually opt for and perhaps even advertise for solution (1), where the driver of its vehicle will be safe as possible and that the A.I. will essentially be a servant to its driver and protect him or her at all costs. Now the issue is clear; although saving more lives appears to be the most resolute and sound solution, at the end of the day, it will be corporate companies deciding how A.I. will be implemented in products such as cars and it is in their best interest to not save the most lives and actually only save the life of its operator. This paradox is an inherent issue which stems from the creation of A.I. that the article lacks to make mention of and as a result will have compounding effects on A.I.’s future.
