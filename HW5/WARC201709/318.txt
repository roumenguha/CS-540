The “Standard One Hundred Year Study on Artificial Intelligence” describes the vision a group of artificial intelligence researchers have of how the world will be shaped by AI by 2030. Many of their points are valid, and the foreseeable improvements are realistic in an ideal world. However, they only briefly touched on a critical component, which would hinder AI development in transportation and healthcare: security. Artificial Intelligence could certainly reach scope the article claims, yet the devices it would interface with provide a challenge.

When it comes to transportation, specifically self-driving cars, security is paramount. All it takes for a potential hacker to cause a fatal accident is one overlooked exploit, and with a self-driving car there are several different attack vectors. If the system is centralized in nature, the server, and server to car communication, is a concern. If the system is decentralized, that is a self-driving system that works peer-to-peer, then there is potential for a malicious car to enter the network and send incorrect data to other cars. All of these challenges would need to be properly addressed, and if a successful attack were to happen it could significantly lower public adoption of the technology out of fear.

When it comes to healthcare there are many ways in which AI could be used to greatly advance the field. For example in the recent Microsoft Imagine Cup 2017, the wining team X.GLU created a smart glucose meter which utilized cloud based machine learning to create a solution that assisted children with managing their diabetes. Solutions like this are a prime example of how AI can help improve the quality of life for many patients in the health care system. However, IoT based devices which rely on cloud computing open new attack vectors, and potentially fatal ones; a hacked smart pacemaker could be used to kill someone. That is an abundant amount of trust to put on both the software running it, and the hardware running the software. 
	
It has repeatedly been shown that IoT devices, which could include both medical related ones and intelligent cars that rely on cloud computing AI, are often riddled with bugs. Tesla cars have been hacked at Def Con 23, and with the recent focus of Def Con on IoT exploits, a variety of IoT devices have been hacked, even simpler things such as smart locks. While the AI running in the cloud may not be vulnerable, the devices it relies on to do actions based on the AI’s evaluations are. Not only is the software a concern, but so is the hardware. At a recent Black Hat USA 2017 presentation called “BREAKING THE X86 INSTRUCTION SET”, a researcher shows how vulnerable processors are, and that the processor manufactures actually have been introducing many undocumented instruction codes for several years, which all have the potential for exploits not known to the public. This has the implication that at this point in time, developers cannot even trust the hardware their code executes on. If there is a bug in hardware, then that bug can propagate up to any software running on it. 

Ultimately the vision of the AI researches have are completely viable in terms of what the AI portion is responsible for doing. The problem arises with the devices that act as the implementers of the AIs decisions. The more they become embedded into society, and even our own bodies, the higher the damage a security exploit could incur, and that is a risk that may hold back adoption, and as a result, development.

