In Stanford University's published "Artificial Intelligence and Life in 2030" article, the authors discuss AI and the inevitable issues it will run into in the United State's court system. In the case of legal troubles, the article states that courts "might arbitrarily assign liability to a human actor even when liability is better located elsewhere for reasons of fairness or efficiency." (Stanford 46). The article goes on to say that in cases where the court can not foresee the harm that the AI caused, liability would fall by default on the victim. In cases of criminal charges, the court would have to prove that there was an intent to commit harm. The authors state how "courts and other legal actors will have to puzzle through whom to hold accountable and on what theory." (Stanford 47). I disagree with the authors regarding the issues that will be presented by introducing AI into the legal system, as responsibility for an AI's actions will fall onto its creators and maintainers because they are the ones responsible for providing and monitoring its training data and testing.

While Artificial Intelligence is often seen as being a sentient being or in some cases equivalent to a human, the AI that we'll be seeing in the near future in many of the technologies discussed in areas like healthcare, education and self-driving cars are not that. They are software with the feature of complex decision making. Like any software, the group or corporation behind maintaining it can be held liable in the case of faulty or subpar performance. Artificial Intelligence is not different. If the maintainers of the AI program document the training material that went into creating the AI, it should not be difficult for investigators to determine whether data for certain cases went into the AI's conception and testing. Based off of that, it should be feasible to determine whether the maintaining group can be held liable in court cases.

New legislation being drafted for Artificial Intelligence also supports software manufacturers being liable for errors caused by the AI. In December 2016, Michigan created the first law pertaining to self-driving cars. Part of the new rules are pertained to liability in the case of car crashes. In an analysis of the new law published by Vox website ReCode, Johana Bhuiyan states how Michigan's Department of Transportation "will require automakers that are operating a ride-hail network to take full liability for accidents in which the vehicle was driving itself and was found at fault." (Bhuiyan).  While this law is fairly vague and only applies to a specific subset of self-driving cars, it sets a precedent that the rest of the country may follow in the coming years.

When the authors of the "Artificial Intelligence and Life in 2030" article speak about how the introduction of AI into society will require laws to change, they are correct. I disagree however on their opinion that AI will cause the need for an overhaul to our laws. New regulations are making the logical step of giving accountability of software to its developers. Over time software developers, like the companies who create physical items, will be held responsible for the items they create. 

Vox article cited: https://www.recode.net/2016/12/9/13890080/michigan-dot-self-driving-cars-laws-automakers 
