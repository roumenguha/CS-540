It's a bad thing to turn a political essay in for an assignment from a tech class, but given that the assignment is to respond to another political essay, here we are.

Maybe in a better world, where regulators had pride in their work, the competence and data-gathering/analyzing techniques to figure out what to do, the freedom to do what was right regardless of whether a sensationalistic media or their political opponents could misrepresent it to persuade illiterate voters to punish them, and the civic virtues to want to do what was right in the first place, the essay's general strategy of "patch existing regulations inside the current framework to handle imminent AI disruptions" would be appropriate.

Instead, we live in a world where page 10 of the panel-of-experts report chose to support a hedging comment on how bad attempts at regulation could actually have bad consequences by citing a New York Times opinion piece that said nothing of the kind (the citation was probably really meant to apply to something further down on page 10). The piece did, however, minimalize the threat of runaway self-improving AI, saying we should redirect any focus on that civilization-ending threat (as an aside, isn't it nice how many potential civilization-ending threats there are right now?) to current petty tribal issues. (Kate Crawford, if a poorly-specified self-improved AI starts taking apart the Earth to build more processors to run on to more efficiently "achieve" its naive programmers' goals, the effects aren't going to be limited to particular demographic groups.)

Even if the report is right and that particular doomsday scenario won't happen, the disruption the report predicts is too big and too soon and too poorly-timed with our unrelated political disintegration to be manageable. We can't just apply simple changes to civil liability to suddenly handle a world where, as the report correctly points out, simple traditional conceptions of "fault" aren't going to apply anymore. We already need changes--we already have a ballooning cost disease problem where healthcare, education, and other areas are structured around avoiding lawsuits, with huge amounts of waste involved. But who's gonna fix it? Not the government any time soon. How in the world, then, will they be able to handle this?

I'm not suggesting that the disruptive power of AI could be put to good use to make the government irrelevant, because even though that would be really nice, it isn't realistic either. We're really just doomed, and the time has come to start deadpools on what the horrifying cosmic force that finally gets us will be. Will it be competition and memetic growths selecting against all our values, like Scott Alexander suggests in his excellent "Meditations on Moloch" essay that you should read right after you read this? Global warming (god knows the government's not going to be able to fix that one either)? Nuclear war with North Korea? Civil war with ourselves? (A year ago, I wouldn't have guessed that Godwin's Law was even in the running, but it's firmly in play now too.)

No, the proper response to future threats to society is to resign ourselves to failure, and then spend our last few decades having fun working on interesting technical problems like those in AI.
