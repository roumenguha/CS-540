While humans have been quick to adopt AI in the forms of curated Facebook feeds, online shopping suggestions, and useful assistants such as SIRI, there are challenges to the future of AI. In the executive summary of Stanford’s One Hundred Year Study on Artificial Intelligence the challenges are listed as the difficulty of creating safe and reliable hardware, the difficulty of interacting with human experts, the challenge of gaining public trust, the challenge of overcoming fears or marginalizing humans, and finally the social and societal risk of diminished interpersonal communications. Each of these challenges represents a key sector of development. The challenge of making safe and reliable hardware is in the area of transportation and service robots. Any hardware that is going to control a car, plane, or some other vehicle is going to have to be much safer than the human equivalent. AI controlled service robots will have to be exceptionally safe, as they will be dealing with young children and the elderly. While it is simple to see the challenges listed by the study as reasonable and just, I believe that the biggest challenge to AI development is government understanding and regulation.

Different government agencies currently regulate different area of AI development. The Federal Aviation Administration is responsible for drones, the Food and Drug Administration for technologies that aid in medical diagnosis and treatment and other agencies as found to be a best fit. There is not a standard established for what levels of safety, performance, and liability are to come with the associated AI. If medical software makes a wrong diagnosis, is it the attending doctor’s fault? Can we assign blame or even immunity of guilt to a software platform that has the combined “virtual” experiences of thousands of doctors and millions of patients? How do we handle a case such as this? After an AI driven car is proven safer than the average human counterpart is, how do we assign fault when two AI cars eventually have an accident with each other? Due to the capital costs associated with gathering the relevant data to develop robust AI systems, the stakes will ultimately be very high.

While the study does note that more “experts” need to be in Government positions to guide and develop future policies as new technologies emerge, the true “experts” will be the ones developing the technologies. The pioneers and entrepreneurs that have led to Google, Facebook, Amazon, Netflix and many others did not have counterparts of equal ability in government roles to guide useful policy. While being able to watch any and all of your favorite TV shows and movies via streaming services certain is substantial change, binge watching Netflix is now something people publically advertise as something they do regularly and enjoy. As AI technologies improve to the point that they are able to offer us things that are either too good for us to resist, or drive us away from other societal interactions will governments have the right experts in the right position? The study already states that children prefer to play games alone to being with their actual friends outside. AI will eventually become too addictive, entertaining, and immersive in our lives. I believe the relevant governments in charge will not have the expertise to help guide us to make the right choices for ourselves. I hope that the large companies that control the data and the intellectual capital to develop such AI act with a well guided ethical and moral compass.

