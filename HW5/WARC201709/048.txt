The Stanford One Hundred Year Study on Artificial Intelligence is considerably comprehensive and detailed on the analysis and prediction of future with the development of AI. However, in the transportation domain, specifically in the smart cars industry, the report may be a little optimistic about its future. 

The first question is that to which extend we can safely or assuring to use the vehicles in a large scale? The article mentions that by the year of 2030, it should prevail but I disagree with this argument. It is more like a non-technical issue. It really depends on how the government will accept the new type of vehicles, which usually takes a long time. Taking Uber, a new business type of rental cars, as an example, Uber was founded at 2009 and is very popular after 2012. However, most governments still do not quite accept this new business type today, in the year of 2017, which is at least 5 years. The main problem about Uber, from a government perspective, is that how Uber, a casual rental car business compared to traditional rental cars, protects its passengers' safety or legal rights. Therefore, the government is very prudential on the topics like safety. Back to the topic of smart cars, the question is how fast can the government accept them when the technology itself is still not mature enough? It is hard to say but it will be very slow. 

There is another issue which could also be fatal and tricky. That is the topic mentioned in the report "Ethical questions arise when programming cars to act in situations in which human injury or death is inevitable, especially when there are split-second choices to be made about whom to put at risk". However, the report did not go into details about it, which may miss a fatal problem inherent in smart cars. Imagine a smart car accident which the car hit a passenger on the street, what should the programmed car do? Do everything to protect the people on the car first or sacrifice the people on the car and save the passenger on the street instead? As a car owner, it is unreasonable to buy a car which will sacrifice its owner's life to save other people but it is also not what passengers want if the smart cars will try to spare passengers' lives to save car owners. If the involved vehicle is not a smart car, then it is simple. The driver can choose to hit the passengers to save his life but also the driver will get punished by his own choice. However, if this happens to smart cars, who do we blame? The driver or the smart car design company? From an intuitive perspective, the smart car design company should be responsible for the accident. However, what choice does the design company have? Driver first or passenger first? As the problem shows, this could damage the industry or the society. In fact, this is an old traditional philosophy question and it is known as "Trolley Problem". It remains to be a tricky ethics problem and no satisfying answer has been given. What could possibly be done to solve this issue may be what MIT did, who gives a test to the society to let the society test themselves to see what consequences do they prefer and the data may have legal usage in the future. 

Based on the government’s prudential decision process and the issue about the morality of smart cars, it is still not imminent for the industry to be flourishing. 
