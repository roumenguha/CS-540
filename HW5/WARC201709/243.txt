      In the One Hundred Year Study on Artificial Intelligence, the Study Panel generally believe that there should be “no cause for concern that AI is an imminent threat to humankind” (Artificial Intelligence and Life in 2030, P.4). Under this general belief, several supporting claims are made. However, this report is overall too optimistic in respect to whether AI will be a threat to human beings, and some of the claims can be challenged. 
      First, the report states that “no machines with self-sustaining long-term goals and intent have been developed, nor are they likely to be developed in the near future”. That is, AI would not be able to set its own goal and hence would not do harm to human beings. However, “self-sustaining long-term goals and intent” may not serve as a good criterion to judge the potential risk of machines with AI turning against humans. It is understandable that a machine with “self-sustaining long-term goals and intent” may develop its own goals that may do harm to human beings, but a machine does not need to have that high-level intelligence to do harm to human beings. A machine with only a specific function but performs that function to an extreme can also be dangerous. Imagine a home-cleaning robot with the short-term goal to clean all the things in the house that may bring illness. Eventually the machine may learn that humans in the house are sources of illness and hence should be “cleaned up”, and harms to humans may then be carried out by the robot. Therefore, a machine does not need self-sustaining long-term goals to be threatful to human beings. So, contrary to the belief of the report, people should not be too optimistic about the nature of AI, but be always mindful of the potential risks of AI. I think the essential point to prevent harms to human beings is to limit the functionality of machines when they are created. Limited functionality may impede the development of Artificial Intelligence, but this manipulation is necessary because AI is developed to benefit human beings, and there is no point to make a fast advancement of AI that may do harm to human beings.
      The report criticizes the FDA’s caution in approving new diagnostic software as the “barrier to rapid innovation” and the HIPAA (Health Insurance Portability and Accountability Act) requirements on privacy as the barrier “to applications that could utilize AI technologies” (P.27). However, those acts are necessary to prevent AI from affecting humans negatively. Just as admitted in the report, even if it’s unlikely that “AI systems will autonomously choose to inflict harm on people, it will be possible for people to use AI-based systems for harmful as well as helpful purposes” (P.10). Without the regulations on software and privacy concerns, AI may develop faster, but the privacy information utilized by AI can also be easily exploited by people with vicious intentions. Hence, those regulations regarded as barriers to development of AI are necessary for the security of individuals, and should not be sacrificed for the development of AI. After all, AI is created to do benefit to human beings, not to make our lives more susceptible to harm.

Reference:
Peter Stone, Rodney Brooks, Erik Brynjolfsson, Ryan Calo, Oren Etzioni, Greg Hager, Julia Hirschberg, Shivaram Kalyanakrishnan, Ece Kamar, Sarit Kraus, Kevin Leyton-Brown, David Parkes, William Press, AnnaLee Saxenian, Julie Shah, Milind Tambe, and Astro Teller.  "Artificial Intelligence and Life in 2030." One Hundred Year Study on Artificial Intelligence: Report of the 2015-2016 Study Panel, Stanford University, Stanford, CA, September 2016. Doc: http://ai100.stanford.edu/2016-report. Accessed:  September 6, 2016.
