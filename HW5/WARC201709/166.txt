The field of artificial intelligence has experienced tremendous growth since its inception, yet no clear definition for intelligence has been agreed upon. "Artificial Intelligence and Life in 2030," a report published as part of the One Hundred Year Study on Artificial Intelligence, states that the "factors [scale, speed, degree of autonomy, and generality]...can be used to evaluate every...instance of intelligence...and to place them at some appropriate location in the spectrum" (Stone et al. 2016).  While these are indeed important factors in evaluating an agent’s intelligence, a key factor is omitted—that of rationality.

Defining rationality as acting in such a way as to achieve the best expected result, any definition of intelligence lacking a rationality component will fail to fully encapsulate the essence of intelligence. This is because under any definition of intelligence that ignores the importance of rationality, how optimally an agent responds will have no bearing on how intelligent it is rated. 

To illustrate the gap this creates, consider two hypothetical agents: Agent A and Agent B. Agents A and B are identical in all but two ways: when confronted with a situation, the expected computation time for Agent A is half that of Agent B, and the expected percentage of situations in which agents A and B react rationally are 10% and 95%, respectively. Using only the criteria proffered by "Artificial Intelligence and Life in 2030," Agent A would be classified as more intelligent than Agent B, simply because it was able to respond quicker; no heed would be paid to the fact that this speed appears to come at the cost of accuracy.

There are few situations in which the ability to act rationally is irrelevant. Much more likely is a situation in which a balance must be struck between computation time and performance. A clear example of this can be found in examining a predator-prey encounter. If the prey reacts to its sensing the predator instantly, simply turning and attempting to outpace the predator, its likelihood of survival is much lower than it would be if it were to attempt some sort of evasive maneuver. Conversely, if it simply remains stationary, trying to determine the best way to escape, it will likely be eaten before a decision is made. Clearly, a balance between speed and calculation is needed. 

In the above case, such a balance has been finely tuned over millions of years by evolution in the form of animals’ instincts. Such instincts allow prey to survey a situation almost instantaneously, and then use a number of heuristic techniques to arrive at a good solution. These heuristics are what help strike the optimal balance between speed and computation. By focusing on just a few key details deemed relevant, and then considering only a handful of all possible alternatives, computation time is significantly reduced, while still producing a response with an expected value much greater than that achieved by chance alone.

The factors mentioned by the Report of the 2015 Study Panel, namely scale, speed, degree of autonomy, and generality, are certainly important. However, by defining intelligence using only these dimensions, the importance of an agent’s accuracy in decision making is left unstated. The importance of rationality is likely implicitly assumed by most programmers, but when crafting what is meant to be a comprehensive model for measuring intelligence, omitting such an important dimension represents a vital flaw in the model.