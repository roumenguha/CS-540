The Stanford One Hundred Year Study on Artificial Intelligence gives interesting insights into AI’s influence on modern society. While the report remains relatively unbiased and recognizes the limitations that the discussed technology has, there are still a couple aspects of the report that I challenge. I base both challenges on my understanding of the magnitude of impact AI could have on our society in the future, and the potential for those impacts to be undesirable. 

Education is a crucial part of society, thus any significant changes to it will have a rippling impact. While technology such as Intelligent Tutoring Systems and Virtual Reality sound promising for the future of education, I think this report is quick to assume that these technologies are in fact pointing society in the right direction in terms of effective education. 

The report says that Natural Language Processing has enabled teachers to “multiply the size of their classrooms while simultaneously addressing individual students’ learning needs and styles.” While this is certainly true, it is not clear that this is truly desirable to make education better. Many people believe that the current status of education is unsatisfactory and has much room for improvement at its core level. Because it is understood that AI cannot fully replace humans in tasks like teaching, I believe that providing virtual teaching aids and robots should not be incorporated until our current education system is satisfactory standing on its own. 

Many of the issues presented, such as being able to double class sizes and assist students better can arguably be resolved by providing more funding and focus on the current state of education. Once this is achieved, I think AI will integrate more effectively and the transition of incorporating technology into education will be less of a risk to such an important aspect of today’s society.

Another aspect of AI technology presented in this report that I challenge is that dealing directly and indirectly with decision-making. While AI in health care has literally saved lives, I ethically struggle with some aspects of technology in the industry. Such an example is in healthcare analytics. I find it incredible that algorithms are able to determine if certain people are at risk for particular medical conditions that may otherwise go unnoticed or admitted by a doctor. This type of data analysis is tricky though. Let’s say that there is an algorithm that determines what risk patients are at for a deathly disease that has a fast onset and is hard to diagnose.  The algorithm does this by determining a risk factor score that is based on electronic health records. In this case, the lives of patients are in the hands of a score that has to have some kind of decision-making threshold. For example, say patients who score a 6.0 or higher from their risk factor score are treated, while anyone lower is admitted. What happens when someone scores just below the flagged threshold and is admitted, but that person still gets fatally sick? 

Although technology like this has the potential to save the lives of more people than ever could be saved before, it is still ethically sensitive when such important decisions based on an AI algorithm determine whether or not someone who may actually be sick goes untreated – even if that person would go untreated when diagnosed by a real doctor. 


