The use of Artificial Intelligence to maintain public safety and security can be applied to a variety purposes. An example stated in Stanford’s One Hundred Year Study on Artificial Intelligence is to monitor largely populated areas to begin to “assist with crime prevention” and “efficiency and efficacy” of busy terminals such as airports. Though the idea of this is very exciting yet there are a couple caveats that must be overcome before proceeding to such solutions as preventing crime. In the Stanford paper Artificial Intelligence’s potential to create a safer world is a future worth pursuing, but in order for that future to be a reality the technology must first overcome its fundamental issues. 
One of these issues encountered is the technological concern of what faces these cameras are able to recognize. A New York Times article written by Kate Crawford helps lead us to the issue’s source. The engineers that are producing the technology and algorithms that detect said faces are predominantly white men. She goes on to explain how this has caused a “data problem,” and that the algorithms being used learn from the images being given to them, images “often chosen by engineers.” A few companies have already found errors recently with this where an algorithm had difficulty tracking or even finding a person with a dark skin tone as well as misreading “images of Asian people as blinking” while they were actually just smiling. These algorithms have difficulties overall with identifying non-white males since they were trained on photos of people who are overwhelmingly white. 
Stanford’s paper presents Artificial Intelligence as future solution to assist with the reduction of racial bias yet recently it has been found that these algorithms have been found to have racial bias too. The algorithm was “twice as likely to mistakenly flag black defendants as being at a higher risk of committing future crimes.” In the same software was “twice as likely to incorrectly flag whites as low risk.” The source of this bias is still unknown but data sets that are skewed to one feature (i.e. skin tone) have been shown to have bias as well.
Bias do not only come from images; they also have been found from crime data. Police departments have been using “historical crime data” to feed to their algorithms to help preemptively patrol areas which are more prone to criminal activity. Though this might sound appropriate and could lead to the reduction of racial bias, it in fact most likely would perpetuate the issue of over policing in areas that already have seen a history of racial bias.
Stanford’s One Hundred Year Study on Artificial Intelligence sets out the ideal, and seemingly reasonable outlook on the capabilities of keeping the public safe. Yet the forthcoming solutions are not reasonable unless there is major change in how data is looked at as well as who is providing said data. The idea of Artificial Intelligence helping to reduce crime activity while having fair judgement is an exciting one, but with the creation and evolution of algorithms that have inherently race driven data sets there is little to no change in sight.

Work Cited 
1.	Crawford, Kate. “AI's White Guy Problem.” The New York Times, The New York Times, 25 June 2016, www.nytimes.com/2016/06/26/opinion/sunday/artificial-intelligences-white-guy-problem.html?mcubz=1.

2.	Peter Stone, Rodney Brooks, Erik Brynjolfsson "Artificial Intelligence and Life in 2030." One Hundred Year Study on Artificial Intelligence: Report of the 2015-2016 Study Panel, Stanford University, Stanford, CA,  September 2016. Doc: http://ai100.stanford.edu/2016-report. Accessed:  September 6, 2016.

