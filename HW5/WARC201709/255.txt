      With many promising and new technologies, government and regulatory groups can be slow to keep up. Money funneled into private research groups and large corporations dominate the field faster than government agencies can respond with regulations for safe and fair use. The article “Artificial Intelligence and Life in 2030” is the result of a long-term study conducted by Stanford University in response to the widespread investments and research into Artificial Intelligence and its applications. It discusses where Artificial Intelligence is headed, but also recommends ways to ensure the safe and fair practices of these technologies as they are integrated into many parts of society. It’s suggestions and implications for regulations and public policy on artificial intelligence seem oversimplified and naïve and ultimately disregard the complexities of government and lobbying corporations and institutions.
      Regulations and public policy, as suggested by the study, must be made by informed personnel in the artificial intelligence and technology sector. While knowledgeable personnel should be behind the regulatory committees and policy, it is naïve to ask government officials to be up to date with all research in artificial intelligence and many other technological fields that are rapidly changing the landscape around us. Although ideal, there simply is not enough funding and resources to educate the government quick enough. That said, regulations, if any to ensure fair practice, are already late in a game where products are far in development cycles and early stages of what artificial intelligence algorithms are capable of are being deployed to help everyday workers. 
      Nonetheless, public policy is a major initiative to ensure the people’s protection from unreasonable and reckless innovations. Self-driving cars are commonly used as an example (as seen within the article/study posted by Stanford University). Many cases already have been reported of accidents of self-driving cars. With little policy deciding blame and responsibility for these incidents, corporations have been able to use loop holes to blame the passengers rather than their own faulty software, algorithms, and machinery (are barely loopholes because they are so blatantly obvious). Policy must be made to protect the people from unfair practice and implementation of artificial intelligence. Handling these grey areas within the legal system will force drastic changes and rulings sooner than we expect as new technologies are release into the world. 
      Another challenge with regulations and policy regard artificial intelligence is that often demanding regulations can inhibit the research and development of new technology. The study contrasted the effects of policy in areas where policy was used to foster positive research practices, such as protecting users’ privacy and personal information, in comparison to areas with stricter regulations, where researchers followed more of a “compliance mentality” and focused on avoiding penalties and fines. There is a fine line between promoting safe research and over-regulating to the point of inhibiting the swift progress of research. The future of artificial intelligence lays in the hand of public officials who have the power to restrict the flow of ideas and their usages. 
      With Elon Musk and other science professionals warning the dangers of artificial intelligence and its potential to become “self-aware” and potentially destructive, it is likely that the prospects highlighted by this study will never fully develop. If public policy is made by uniformed government officials who fear artificial intelligence more than they understand it, harsh restrictions could hinder research and create this “compliance mentality” that forces industry conglomerates and private researchers to dance around regulation rather than protecting their consumers and ensuring the safety and reliability of their products. 
