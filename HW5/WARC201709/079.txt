Diminished AI moral problems   The article “Artificial Intelligence and Life in 2030” mainly introduces the history and current progress and obstacles in artificial intelligence(AI) field. It can be seen from the article that AI can have a large range of application though there are still obstacles like the invasion of privacy and low-employment situation after several positions are replaced by AI. Yet there are some moral aspects that the article failed to address that also requires a solution. The “uncanny valley” hypothesis and moral problems such as the trolley problem that the autonomous cars need to resolve.       According to the article, AI field can be summarized into two general categories, machines that require less interacts with human, such as autonomous car, and robots that are required to communicate with human, including part of the educational AI. For the AI that needs to involve with real human beings, the article talked about financial and technical issues that encumber the popularize of such robots, while it failed to discuss a psychological problem that human-like robots themselves can cause discomfort to public. A hypothesis called “the uncanny valley” is studied recent years as AI becomes more common. The uncanny valley hypothesis stressed that when observing a human-like robot or other object, negative feelings such as nausea may be triggered. In fact, research has proved that human-like android faces can cause trust issues between human and the robot (Mathur et al., 2016). Despite of the problem itself, there has not been an effective method to determine how likely should robots be in order to serve people perfectly. It may seem inconspicuous today, yet it still exists and when robots really begin to interact with human in the future, the uncanny valley could have a large impact on the relationship between human and AI. 	Despite of the uncanny valley effect coming from AI with human-like appearances, the autonomous car also faces moral barrier which the article does not focus on. When human drivers are replaced by auto-driving, a significant conflict is that who is in fact in charge of the car and who is responsible for all the accidents that could happen to the self-driving car. There is one solution given in the article which is that the car is only autonomous when the human driver’s hands are on the wheel. Besides the fact that this is not the perfect solution since accidents can still happens as proved in the article afterwards, it is also not able to solve some of the moral problem, such as “the trolley problem”. When a train is going to kill five people standing on the tracks while you are the only person who can switch the train to a side track, on which there is only one person. Even for real human, there is no certain or correct answer to the problem. Similarly, when there is an unavoidable accident and only the driver or the pedestrian can survive, it is hard to tell which choice should the car choose (“Why”, 2015). When a human driver is in the car, his or her decision is usually understandable since it concerns his or her own life. On the other hand, the programmer may not be forgiven for designing a car that takes a life. Situations like the trolley problem is not usual but still can appear. As a designer, making the choice for the customer is where the hard part is. 	Moral issue is an enormous challenge for AI. Maybe other barriers like the privacy issue, can be solved by a line of code, moral issues have no correct answer or an exact boundary. Maybe one day the moral criterion itself will be modified to adapt the necessity of the society, but for now, these questions still remained unsolved and requires more consideration. ReferenceMathur, M.B., & Reichling, D.B. (2016). Navigating a social world with robot partners: A quantitative cartography of the Uncanny Valley. Cognition, 146, 22-32.Why Self-Driving Cars Must Be Programmed to Kill. (2015). Retrieved from https://www.technologyreview.com/s/542626/why-self-driving-cars-must-be-programmed-to-kill/