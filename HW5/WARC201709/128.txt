	Implementing AI in security: A good idea? 

	There have been recent discussions on the use of artificial intelligence (AI) to increase public safety and security, and whether this will have an effective and non-abusive outcome. Some experts claim using AI to enhance public safety has a great potential, as police officers will be able to identify and prevent crimes more easily. While it is true this may have a potential for good, it may also give resources for institutions to abuse this system and target groups in a discriminatory way. This possibility may slow the implementation of artificial intelligence to overview security in our society until safer systems are developed. Using AI to increase public safety and security in the future is bound to have negative outcomes if systems to prevent abuse and discrimination are not developed.

	Some people might argue that using AI to enhance public safety can lead to better crime prevention. According to the first report of the "One Hundred Year Study on Artificial Intelligence” made by Stanford University (2016), artificial intelligence will, by 2030, be able to assist crime prevention by constantly identifying people on video surveillance cameras, and discerning between events to detect abnormalities and warn police authorities. This attest to the great potential of implementing AI in security systems to increase security worldwide.

	While some might believe implementing artificial intelligence to security systems can only have benefits, they might discriminate people when searching for abnormalities. According to Dr. Mahesh Saptharishi, Senior Vice President of Analytics and Data Science for Avigilon (a video surveillance company), current surveillance systems detect humans and vehicles and take feedback from the operator to discern which events are normal and which are uncommon; the system then saves the feedback to improve its recognition (2014). This shows a current approach on surveillance cameras’ artificial intelligence that will possibly be implemented into public security cameras in the future. However, this system builds its data from humans. Sam DeBrule, co-founder of the journal “Machine Learnings”, states that artificial intelligence systems that build upon human judgement can perpetuate biases (2017). This means that security systems that use artificial intelligence built upon human decisions might become discriminative to certain groups of people. 

	Apart from possibly discriminating certain groups of people when searching for abnormalities, institutions might also abuse security systems with artificial intelligence. Barry Steinhardt, director of the American Civil Liberties Union Project on Technology and Liberty, states that the FBI, and other police departments in the US, have conducted illegal operations during the Vietnam War to investigate and persecute those against racial segregation and the war (2002). This shows how prone are institutions to abuse new technologies, specially those that can constantly identify people in public spaces, and carry illegal practices in times of political turmoil.

	The outcome of integrating AI to public security and safety is being widely discussed at the moment. It has, however, been shown that this new technology can suffer from institutional abuse and target groups of people in a discriminatory way. While some might argue artificial intelligence has a great potential for improving security, without safer systems to combat misuse and biased targeting, it will be incredibly hard for artificial intelligence to reach mainstream use in public security systems in the coming future.